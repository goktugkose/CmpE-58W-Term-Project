{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adv RE - Prioritization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ADg_qjc6LXE",
        "outputId": "52ae5dfe-8296-453c-bb8a-e94ba153bb2c"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp37-none-any.whl size=829180944 sha256=1981a5e39302be9a1a7e70202e9526c2e0fd4e154e5c1edf70f74e71bb88fc61\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bjnv320e/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGsn0FKeU31C",
        "outputId": "dad15f0a-d72a-439b-f829-7c6e7b44583a"
      },
      "source": [
        "!pip install keybert\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import re \n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "import spacy\n",
        "import en_core_web_lg\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import keybert\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from keybert import KeyBERT\n",
        "sentence_model = SentenceTransformer('paraphrase-distilroberta-base-v2')\n",
        "model = KeyBERT(model=sentence_model)\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keybert in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keybert) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.19.5)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.2.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.6.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.8.1+cu101)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.1.95)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.9.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (0.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keybert) (2.4.7)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nOGELUydge5"
      },
      "source": [
        "def retrieve_data(fileName):\n",
        "  # Opening JSON file\n",
        "  f = open(fileName)\n",
        "  # returns JSON object as a dictionary\n",
        "  data = json.load(f)\n",
        "  return data\n",
        "\n",
        "def extract_review_and_label(data, label):\n",
        "  labels = [label] * len(data)\n",
        "  reviews = [d['stopwords_removal_lemmatization'] for d in data]\n",
        "  return reviews, labels  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKuFxOcadp8r"
      },
      "source": [
        "feature_data = retrieve_data(\"Feature.json\")\n",
        "feature_reviews, feature_labels = extract_review_and_label(feature_data, 0)\n",
        "\n",
        "user_experience_data = retrieve_data(\"UserExperience.json\")\n",
        "user_experience_reviews, user_experience_labels = extract_review_and_label(user_experience_data, 1)\n",
        "\n",
        "rating_data = retrieve_data(\"Rating.json\")\n",
        "rating_reviews, rating_labels = extract_review_and_label(rating_data, 2)\n",
        "\n",
        "bug_data = retrieve_data(\"Bug.json\")\n",
        "bug_reviews, bug_labels = extract_review_and_label(bug_data, 3)\n",
        "\n",
        "reviews = np.concatenate(([],bug_reviews))\n",
        "labels = np.concatenate(([],bug_labels))\n",
        "\n",
        "randomize = np.arange(len(reviews))\n",
        "np.random.shuffle(randomize)\n",
        "reviews = reviews[randomize]\n",
        "labels = labels[randomize]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxhrYaLifhRS"
      },
      "source": [
        "def preprocess(review):\n",
        "\n",
        "  temp = \"\"\n",
        "  lst = []\n",
        "  temp = re.sub('[^a-zA-Z]',' ',review).strip()\n",
        "  temp = temp.lower()\n",
        "  return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DnBW8otfi8P"
      },
      "source": [
        "results = []\n",
        "for r in reviews:\n",
        "  results.append(preprocess(r))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DyKx0hx2flka",
        "outputId": "4622e485-6f00-4181-b568-6833a9215aaa"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(results, columns=[\"sentence\"])\n",
        "df[\"label\"] = labels\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ever since io   update  dropbox have not work ...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>previous version  if user be look at picture s...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>seriouslywhy do select all functionality no lo...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nice nice app  enjoy but kid have more fun wis...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>constant crash io     crash    minute evernote...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  label\n",
              "0  ever since io   update  dropbox have not work ...    3.0\n",
              "1  previous version  if user be look at picture s...    3.0\n",
              "2  seriouslywhy do select all functionality no lo...    3.0\n",
              "3  nice nice app  enjoy but kid have more fun wis...    3.0\n",
              "4  constant crash io     crash    minute evernote...    3.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvu-TLSq8Y1G"
      },
      "source": [
        "def extract_keywords(str_):\n",
        "  keywords = model.extract_keywords(str_, keyphrase_ngram_range=(2,3), use_mmr=True, diversity=0.3, stop_words='english')\n",
        "  lst = []\n",
        "  for k,v in keywords:\n",
        "    for chunk in list(k.split()):\n",
        "      if len(str(chunk).split()) > 0:\n",
        "        for el in str(chunk).split():\n",
        "          lst.append(el) if el not in lst else 1\n",
        "      else:\n",
        "        lst.append(chunk) if chunk not in lst else 1\n",
        "  return \" \".join(lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfV0HBxIbO0X"
      },
      "source": [
        "def extract_bigrams(str_):\n",
        "  keywords = model.extract_keywords(str_, keyphrase_ngram_range=(2,2), use_mmr=True, diversity=0.3, stop_words='english',top_n=10)\n",
        "  lst = []\n",
        "  for k,v in keywords:\n",
        "    lst.append(tuple(k.split()))    \n",
        "  return lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhoYAYcbiRvC"
      },
      "source": [
        "def extract_trigrams(str_):\n",
        "  keywords = model.extract_keywords(str_, keyphrase_ngram_range=(3,3), use_mmr=True, diversity=0.3, stop_words='english',top_n=5)\n",
        "  lst = []\n",
        "  for k,v in keywords:\n",
        "    lst.append(tuple(k.split()))    \n",
        "  return lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDZVz85I_XEr"
      },
      "source": [
        "doc_vectors = []\n",
        "for review in df.sentence:\n",
        "  vectors = []\n",
        "  doc_vectors.append(extract_bigrams(review))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z38Eio8aYD05"
      },
      "source": [
        "import itertools \n",
        "\n",
        "fdist = nltk.FreqDist(list(itertools.chain(*doc_vectors)))\n",
        "common_bigrams = sorted(fdist.items(), key=lambda x: (-x[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYOmrJyieUJe"
      },
      "source": [
        "lst_final = []\n",
        "cmn_bigrams = [bigram for bigram in common_bigrams if bigram[1] > 1]\n",
        "for bigram in cmn_bigrams:\n",
        "  ctr = -1\n",
        "  for lst in doc_vectors:\n",
        "    ctr+=1\n",
        "    if bigram[0] in lst:\n",
        "      lst_final.append(extract_trigrams(df.sentence[ctr]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7b-hTZbip6p",
        "outputId": "cf1e15dd-791c-4827-91e0-50a5614cea89"
      },
      "source": [
        "lst_words = []\n",
        "for a,b,c in set(list(itertools.chain(*lst_final))):\n",
        "  if (\"VB\" in pos_tag([a])[0][1]  or \"VB\" in pos_tag([b])[0][1] or \"VB\" in pos_tag([c])[0][1]) and not (\"VB\" in pos_tag([a])[0][1]  and \"VB\" in pos_tag([b])[0][1] and \"VB\" in pos_tag([c])[0][1]) and  (a in words.words() and b in words.words() and c in words.words()):\n",
        "    phrase_list = list(nlp(\" \".join([a,b,c])).noun_chunks)\n",
        "    #print([a,b,c])\n",
        "    print(phrase_list) if len(phrase_list) >= 1 and len(str(phrase_list[0]).split()) > 1 else print(\" \".join([a,b,c]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hat come half\n",
            "access recipe save\n",
            "[extremely slow problem]\n",
            "[memory program]\n",
            "search come fix\n",
            "don know yahoo\n",
            "[type need]\n",
            "[bar slow crash]\n",
            "buy night try\n",
            "[slow process]\n",
            "action painfully slow\n",
            "time make search\n",
            "[file photo]\n",
            "[use icon]\n",
            "wont work say\n",
            "adjust moon happen\n",
            "start irritate slow\n",
            "time try save\n",
            "consider book file\n",
            "image disappear happen\n",
            "add button selected\n",
            "[column flicker]\n",
            "[change password]\n",
            "[exact video]\n",
            "bug make impossible\n",
            "[overwrite note]\n",
            "[manage trip]\n",
            "update make change\n",
            "[preview word]\n",
            "doesnt save favorite\n",
            "doesnt save progress\n",
            "[half hat]\n",
            "update whats happen\n",
            "review buy sound\n",
            "update make harder\n",
            "[brightness display]\n",
            "save need fixed\n",
            "[point work]\n",
            "[new update]\n",
            "update lose note\n",
            "[happen month fix]\n",
            "[map distance]\n",
            "[file list]\n",
            "game doesnt save\n",
            "hat make want\n",
            "work add button\n",
            "[fix problem]\n",
            "search slow time\n",
            "io make legacy\n",
            "io know bug\n",
            "[far restaurant]\n",
            "say update fix\n",
            "phone like come\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK3JuOdTDvN7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}